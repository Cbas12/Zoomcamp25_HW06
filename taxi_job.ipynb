{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "from pyflink.table import EnvironmentSettings, DataTypes, TableEnvironment, StreamTableEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_taxi_events_sink_postgres(t_env):\n",
    "    table_name = 'taxi_events_postgres'\n",
    "    sink_ddl = f\"\"\"\n",
    "        CREATE OR REPLACE TABLE {table_name} (\n",
    "            lpep_pickup_datetime VARCHAR,\n",
    "            lpep_dropoff_datetime VARCHAR,\n",
    "            PULocationID INTEGER,\n",
    "            DOLocationID INTEGER,\n",
    "            passenger_count DOUBLE,\n",
    "            trip_distance DOUBLE,\n",
    "            tip_amount DOUBLE\n",
    "        ) WITH (\n",
    "            'connector' = 'jdbc',\n",
    "            'url' = 'jdbc:postgresql://postgres:5432/postgres',\n",
    "            'table-name' = '{table_name}',\n",
    "            'username' = 'postgres',\n",
    "            'password' = 'postgres',\n",
    "            'driver' = 'org.postgresql.Driver'\n",
    "        );\n",
    "        \"\"\"\n",
    "    t_env.execute_sql(sink_ddl)\n",
    "    return table_name\n",
    "\n",
    "def create_events_source_kafka(t_env):\n",
    "    table_name = \"taxi_events_kafka\"\n",
    "    pattern = \"yyyy-MM-dd HH:mm:ss\"\n",
    "    source_ddl = f\"\"\"\n",
    "        CREATE or replace TABLE {table_name} (\n",
    "            lpep_pickup_datetime VARCHAR,\n",
    "            lpep_dropoff_datetime VARCHAR,\n",
    "            PULocationID INTEGER,\n",
    "            DOLocationID INTEGER,\n",
    "            passenger_count DOUBLE,\n",
    "            trip_distance DOUBLE,\n",
    "            pickup_timestamp AS TO_TIMESTAMP(lpep_pickup_datetime, '{pattern}'),\n",
    "            WATERMARK FOR pickup_timestamp AS pickup_timestamp - INTERVAL '15' SECOND\n",
    "        ) WITH (\n",
    "            'connector' = 'kafka',\n",
    "            'properties.bootstrap.servers' = 'redpanda-1:29092',\n",
    "            'topic' = 'green-trips',\n",
    "            'scan.startup.mode' = 'earliest-offset',\n",
    "            'properties.auto.offset.reset' = 'earliest',\n",
    "            'format' = 'json'\n",
    "        );\n",
    "        \"\"\"\n",
    "    t_env.execute_sql(source_ddl)\n",
    "    return table_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_processing():\n",
    "    # Set up the execution environment\n",
    "    env = StreamExecutionEnvironment.get_execution_environment()\n",
    "    env.enable_checkpointing(10 * 1000)\n",
    "    # env.set_parallelism(1)\n",
    "\n",
    "    # Set up the table environment\n",
    "    settings = EnvironmentSettings.new_instance().in_streaming_mode().build()\n",
    "    t_env = StreamTableEnvironment.create(env, environment_settings=settings)\n",
    "    try:\n",
    "        # Create Kafka table\n",
    "        source_table = create_events_source_kafka(t_env)\n",
    "        postgres_sink = create_taxi_events_sink_postgres(t_env)\n",
    "        # write records to postgres too!\n",
    "        t_env.execute_sql(\n",
    "            f\"\"\"\n",
    "                    INSERT INTO {postgres_sink}\n",
    "                    SELECT\n",
    "                        *\n",
    "                    FROM {source_table}\n",
    "                    \"\"\"\n",
    "        ).wait()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Writing records from Kafka to JDBC failed:\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing records from Kafka to JDBC failed: An error occurred while calling o226.executeSql.\n",
      ": org.apache.flink.table.api.ValidationException: Unable to create a source for reading table 'default_catalog.default_database.taxi_events_kafka'.\n",
      "\n",
      "Table options are:\n",
      "\n",
      "'connector'='kafka'\n",
      "'format'='json'\n",
      "'properties.auto.offset.reset'='earliest'\n",
      "'properties.bootstrap.servers'='redpanda-1:29092'\n",
      "'scan.startup.mode'='earliest-offset'\n",
      "'topic'='green-trips'\n",
      "\tat org.apache.flink.table.factories.FactoryUtil.createDynamicTableSource(FactoryUtil.java:235)\n",
      "\tat org.apache.flink.table.factories.FactoryUtil.createDynamicTableSource(FactoryUtil.java:260)\n",
      "\tat org.apache.flink.table.planner.plan.schema.CatalogSourceTable.createDynamicTableSource(CatalogSourceTable.java:175)\n",
      "\tat org.apache.flink.table.planner.plan.schema.CatalogSourceTable.toRel(CatalogSourceTable.java:115)\n",
      "\tat org.apache.calcite.sql2rel.SqlToRelConverter.toRel(SqlToRelConverter.java:4034)\n",
      "\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertIdentifier(SqlToRelConverter.java:2904)\n",
      "\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2464)\n",
      "\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2378)\n",
      "\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2323)\n",
      "\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectImpl(SqlToRelConverter.java:730)\n",
      "\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertSelect(SqlToRelConverter.java:716)\n",
      "\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertQueryRecursive(SqlToRelConverter.java:3880)\n",
      "\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertQuery(SqlToRelConverter.java:620)\n",
      "\tat org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$rel(FlinkPlannerImpl.scala:233)\n",
      "\tat org.apache.flink.table.planner.calcite.FlinkPlannerImpl.rel(FlinkPlannerImpl.scala:208)\n",
      "\tat org.apache.flink.table.planner.operations.SqlNodeConvertContext.toRelRoot(SqlNodeConvertContext.java:77)\n",
      "\tat org.apache.flink.table.planner.operations.converters.SqlQueryConverter.convertSqlNode(SqlQueryConverter.java:48)\n",
      "\tat org.apache.flink.table.planner.operations.converters.SqlNodeConverters.convertSqlNode(SqlNodeConverters.java:85)\n",
      "\tat org.apache.flink.table.planner.operations.SqlNodeToOperationConversion.convertValidatedSqlNode(SqlNodeToOperationConversion.java:271)\n",
      "\tat org.apache.flink.table.planner.operations.SqlNodeToOperationConversion.convertValidatedSqlNodeOrFail(SqlNodeToOperationConversion.java:387)\n",
      "\tat org.apache.flink.table.planner.operations.SqlNodeToOperationConversion.convertSqlInsert(SqlNodeToOperationConversion.java:774)\n",
      "\tat org.apache.flink.table.planner.operations.SqlNodeToOperationConversion.convertValidatedSqlNode(SqlNodeToOperationConversion.java:350)\n",
      "\tat org.apache.flink.table.planner.operations.SqlNodeToOperationConversion.convert(SqlNodeToOperationConversion.java:261)\n",
      "\tat org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:106)\n",
      "\tat org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:728)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.flink.table.api.ValidationException: Cannot discover a connector using option: 'connector'='kafka'\n",
      "\tat org.apache.flink.table.factories.FactoryUtil.enrichNoMatchingConnectorError(FactoryUtil.java:814)\n",
      "\tat org.apache.flink.table.factories.FactoryUtil.discoverTableFactory(FactoryUtil.java:788)\n",
      "\tat org.apache.flink.table.factories.FactoryUtil.createDynamicTableSource(FactoryUtil.java:231)\n",
      "\t... 35 more\n",
      "Caused by: org.apache.flink.table.api.ValidationException: Could not find any factory for identifier 'kafka' that implements 'org.apache.flink.table.factories.DynamicTableFactory' in the classpath.\n",
      "\n",
      "Available factory identifiers are:\n",
      "\n",
      "blackhole\n",
      "datagen\n",
      "filesystem\n",
      "print\n",
      "python-input-format\n",
      "\tat org.apache.flink.table.factories.FactoryUtil.discoverFactory(FactoryUtil.java:624)\n",
      "\tat org.apache.flink.table.factories.FactoryUtil.enrichNoMatchingConnectorError(FactoryUtil.java:810)\n",
      "\t... 37 more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    log_processing()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
