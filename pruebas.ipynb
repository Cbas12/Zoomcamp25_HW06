{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Consumer, KafkaError, KafkaException\n",
    "\n",
    "# Configuration\n",
    "conf = {\n",
    "    'bootstrap.servers': 'localhost:9092',  # Redpanda broker address\n",
    "    'group.id': 'python-consumer-group',   # Consumer group ID\n",
    "    'auto.offset.reset': 'earliest'        # Start reading from the beginning of the topic\n",
    "}\n",
    "\n",
    "# Create a consumer instance\n",
    "consumer = Consumer(conf)\n",
    "\n",
    "# Subscribe to the topic\n",
    "consumer.subscribe(['green-trips'])\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        # Poll for messages\n",
    "        msg = consumer.poll(timeout=1.0)  # Wait for 1 second for a message\n",
    "        if msg is None:\n",
    "            continue  # No message received\n",
    "        if msg.error():\n",
    "            if msg.error().code() == KafkaError._PARTITION_EOF:\n",
    "                # End of partition event\n",
    "                print(f\"Reached end of partition {msg.partition()}\")\n",
    "            else:\n",
    "                raise KafkaException(msg.error())\n",
    "        else:\n",
    "            # Successfully consumed a message\n",
    "            print(f\"Consumed message: {msg.value().decode('utf-8')} (Partition: {msg.partition()}, Offset: {msg.offset()})\")\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Consumer interrupted.\")\n",
    "finally:\n",
    "    # Close the consumer\n",
    "    consumer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "from pyflink.datastream.connectors import FlinkKafkaConsumer\n",
    "from pyflink.datastream.connectors.jdbc import JdbcSink, JdbcConnectionOptions, JdbcExecutionOptions\n",
    "from pyflink.common.serialization import SimpleStringSchema\n",
    "from pyflink.common import WatermarkStrategy, Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Kafka source\n",
    "def create_kafka_source():\n",
    "    return FlinkKafkaConsumer(\n",
    "        topics='green-trips',  # Kafka topic\n",
    "        deserialization_schema=SimpleStringSchema(),  # Deserialize messages as strings\n",
    "        properties={\n",
    "            'bootstrap.servers': 'localhost:9092',  # Redpanda broker address\n",
    "            'group.id': 'pyflink-consumer-group'    # Consumer group ID\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Define the PostgreSQL sink\n",
    "def create_postgres_sink():\n",
    "    return JdbcSink.sink(\n",
    "        sql=\"INSERT INTO green_trips (trip_data) VALUES (?)\",  # SQL query\n",
    "        type_info=Types.ROW([Types.STRING()]),  # Data type (single string column)\n",
    "        jdbc_connection_options=JdbcConnectionOptions.JdbcConnectionOptionsBuilder()\n",
    "            .with_url('jdbc:postgresql://localhost:5432/postgres')  # PostgreSQL URL\n",
    "            .with_driver_name('org.postgresql.Driver')  # PostgreSQL driver\n",
    "            .with_user_name('postgres')  # PostgreSQL username\n",
    "            .with_password('postgres')  # PostgreSQL password\n",
    "            .build(),\n",
    "        jdbc_execution_options=JdbcExecutionOptions.builder()\n",
    "            .with_batch_size(100)  # Batch size for inserts\n",
    "            .build()\n",
    "    )\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # Set up the execution environment\n",
    "    env = StreamExecutionEnvironment.get_execution_environment()\n",
    "    env.add_jars(\n",
    "        \"file:///opt/flink/lib/flink-connector-kafka_2.12-1.16.0.jar\",\n",
    "        \"file:///opt/flink/lib/flink-connector-jdbc_2.12-1.16.0.jar\",\n",
    "        \"file:///opt/flink/lib/postgresql-42.6.0.jar\"\n",
    "    )\n",
    "\n",
    "    # Create the Kafka source\n",
    "    kafka_source = create_kafka_source()\n",
    "    kafka_source.set_start_from_earliest()  # Start consuming from the earliest offset\n",
    "\n",
    "    # Add the Kafka source to the environment\n",
    "    stream = env.add_source(kafka_source)\n",
    "\n",
    "    # Transform the data (if needed)\n",
    "    # For example, convert the string message to a Row object\n",
    "    transformed_stream = stream.map(\n",
    "        lambda message: Row(message),  # Wrap the message in a Row object\n",
    "        output_type=Types.ROW([Types.STRING()])\n",
    "    )\n",
    "\n",
    "    # Add the PostgreSQL sink\n",
    "    transformed_stream.add_sink(create_postgres_sink())\n",
    "\n",
    "    # Execute the job\n",
    "    env.execute(\"Redpanda to PostgreSQL Job\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
